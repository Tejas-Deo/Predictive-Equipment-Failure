{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:8080/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [03/Dec/2020 09:26:56] \"\u001b[37mGET /index HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (1, 7)\n",
      "[IterativeImputer] Ending imputation round 1/4, elapsed time 0.12\n",
      "[IterativeImputer] Ending imputation round 2/4, elapsed time 0.23\n",
      "[IterativeImputer] Ending imputation round 3/4, elapsed time 0.35\n",
      "[IterativeImputer] Ending imputation round 4/4, elapsed time 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [03/Dec/2020 09:27:11] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import impyute\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "\n",
    "from pickle import load\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "import flask\n",
    "Predictive_equipment_failure_model_productionization = Flask(__name__)\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "TO STORE THE COLUMN NAMES AS THEY ARE USEFUL TO EXTRACT THE DATAFRAMES WHILE DOING DATA PRE-PROCESSING\n",
    "'''\n",
    "column_names_less_than_5_percent = ['sensor9_measure','sensor12_measure','sensor13_measure','sensor16_measure','sensor17_measure','sensor18_measure','sensor28_measure','sensor31_measure','sensor33_measure','sensor34_measure','sensor35_measure','sensor49_measure','sensor59_measure','sensor60_measure','sensor89_measure','sensor90_measure','sensor98_measure','sensor99_measure', 'sensor12_into_sensor13_measure', 'sensor12_minus_sensor13_measure', 'sensor35_into_sensor17_measure']\n",
    "\n",
    "\n",
    "column_names_for_etr_imputer = ['sensor3_measure', 'sensor56_measure','sensor62_measure','sensor81_measure','sensor82_measure','sensor103_measure', 'sensor81_into_sensor82_measure']\n",
    "\n",
    "\n",
    "column_names_for_ridge_regression_imputer = ['sensor36_measure']\n",
    "\n",
    "\n",
    "top_25_column_names = ['sensor9_measure','sensor12_measure','sensor13_measure','sensor16_measure','sensor17_measure','sensor18_measure','sensor28_measure','sensor31_measure','sensor33_measure','sensor34_measure','sensor35_measure','sensor49_measure','sensor59_measure','sensor60_measure','sensor89_measure','sensor90_measure','sensor98_measure','sensor99_measure','sensor3_measure', 'sensor56_measure','sensor62_measure','sensor81_measure','sensor82_measure','sensor103_measure', 'sensor36_measure']\n",
    "\n",
    "\n",
    "original_df = pd.read_csv(r'equip_failures_training_set.csv')\n",
    "\n",
    "original_df.drop(['id', 'target'], axis = 1, inplace=True)\n",
    "\n",
    "column_names_for_input_data = original_df.columns\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "TO START LOADING THE DATA PRE-PROCESSING MODELS AND OTHER MODELS\n",
    "'''\n",
    "etr_itr_imputer = load(open(r'etr_itr_imputer.pkl', 'rb'))\n",
    "\n",
    "ridge_regression_imputer = load(open(r'ridge_regression_imputer.pkl', 'rb'))\n",
    "\n",
    "measures_data_scaler = load(open(r'measures_data_scaler.pkl', 'rb'))\n",
    "\n",
    "histogram_data_scaler = load(open(r'histogram_data_scaler.pkl', 'rb'))\n",
    "\n",
    "\n",
    "# TO LOAD THE BASE MODELS AND STORE THEM IN A LIST\n",
    "loaded_base_models_list = []\n",
    "\n",
    "for i in range(500):   # as there are 500 base models\n",
    "    model = load(open(r'decision_tree_{}.pkl'.format(i), 'rb'))\n",
    "    loaded_base_models_list.append(model)\n",
    "\n",
    "    \n",
    "meta_classifier = load(open(r'xgboost_model.pkl', 'rb'))\n",
    "\n",
    "column_names_per_base_model = load(open(r'column_names_for_each_base_model.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "TO CLEAN THE DATA AND PERFORM FEATURE ENGINEERING AND RETURN THE FINAL DATAPOINT WHICH WILL BE THE INPUT FOR THE MODEL\n",
    "'''\n",
    "def clean_input_data(input_data, etr_itr_imputer, ridge_regression_imputer, histogram_data_scaler, loaded_base_models_list, meta_classifier, column_names_less_than_5_percent, column_names_for_etr_imputer, column_names_for_ridge_regression_imputer, measures_data_scaler, column_names_per_base_model, top_25_column_names):\n",
    "\n",
    "    # to replace the \"na\" values with \"np.NAN\" values\n",
    "    input_data = input_data.replace('na', np.NAN)\n",
    "\n",
    "    \n",
    "    # as all the values are numbers, to convert their data-type into float\n",
    "    input_data = input_data.astype('float')\n",
    "\n",
    "    \n",
    "    '''\n",
    "    to extract required column names from the input_data and perform the required data-preprocessing tasks on\n",
    "    respective data\n",
    "    '''\n",
    "    # to separate measures data and histogram data\n",
    "    measures_df_column_names = []\n",
    "    histogram_df_column_names = []\n",
    "\n",
    "    for i in input_data.columns:\n",
    "        if \"histogram\" not in i:\n",
    "            measures_df_column_names.append(i)\n",
    "        else:\n",
    "            histogram_df_column_names.append(i)\n",
    "\n",
    "    # to extract the data\n",
    "    measures_data = input_data[measures_df_column_names]\n",
    "    histogram_data = input_data[histogram_df_column_names]\n",
    "\n",
    "\n",
    "    # to extract the 'top_25_column_names' from the measures data\n",
    "    measures_data = measures_data[top_25_column_names]\n",
    "\n",
    "    \n",
    "    # to add the engineered features\n",
    "    measures_data['sensor12_into_sensor13_measure'] =  measures_data['sensor12_measure'] * measures_data['sensor13_measure']\n",
    "    measures_data['sensor12_minus_sensor13_measure'] = measures_data['sensor12_measure'] - measures_data['sensor13_measure']\n",
    "    measures_data['sensor35_into_sensor17_measure'] = measures_data['sensor35_measure'] * measures_data['sensor17_measure']\n",
    "    measures_data['sensor81_into_sensor82_measure'] = measures_data['sensor81_measure'] * measures_data['sensor82_measure']\n",
    "\n",
    "    \n",
    "    # to extract the column names from measures data at this step\n",
    "    revised_measures_data_column_names = measures_data.columns\n",
    "\n",
    "    \n",
    "    # to transform the \"measures_data\" using \"measures_data_scaler\"\n",
    "    measures_data = measures_data_scaler.transform(measures_data)\n",
    "\n",
    "    \n",
    "    # to convert it into DATAFRAME again and assign column names\n",
    "    measures_data = pd.DataFrame(measures_data)\n",
    "    measures_data.columns = revised_measures_data_column_names\n",
    "\n",
    "\n",
    "    # to extract the corresponding column names from the measures data and impute the missing values\n",
    "    measures_df_less_than_5_percent = measures_data[column_names_less_than_5_percent]\n",
    "    measures_df_less_than_30_percent = measures_data[column_names_for_etr_imputer]\n",
    "    measures_df_less_than_75_percent = measures_data[column_names_for_ridge_regression_imputer]\n",
    "\n",
    "    \n",
    "    # to replace the NAN values from the \"measures_dfLess_than_5_percent\" by 0\n",
    "    measures_df_less_than_5_percent = measures_df_less_than_5_percent.replace(np.NAN, 0)\n",
    "\n",
    "    \n",
    "    # to extract the column names\n",
    "    measures_df_less_than_30_percent_column_names = measures_df_less_than_30_percent.columns\n",
    "    measures_df_less_than_75_percent_column_names = measures_df_less_than_75_percent.columns\n",
    "\n",
    "\n",
    "    # to use EXTRATREESREGRESSOR for data imputation for \"measures_df_less_than_30_percent\"\n",
    "    transformed_measures_df_less_than_30_percent = etr_itr_imputer.transform(measures_df_less_than_30_percent)\n",
    "\n",
    "\n",
    "    # to use RIDGE REGRESSION IMPUTER for data imputation for \"measures_df_less_than_75_percent\"\n",
    "    transformed_measures_df_less_than_75_percent = ridge_regression_imputer.transform(measures_df_less_than_75_percent)\n",
    "\n",
    "\n",
    "    # to CONVERT the \"measures_df_less_than_30_percent\" and \"measures_df_less_than_75_percent\" into DATAFRAMES\n",
    "    transformed_measures_df_less_than_30_percent = pd.DataFrame(transformed_measures_df_less_than_30_percent, index=measures_df_less_than_5_percent.index)\n",
    "    transformed_measures_df_less_than_75_percent = pd.DataFrame(transformed_measures_df_less_than_75_percent, index=measures_df_less_than_5_percent.index)\n",
    "\n",
    "    \n",
    "    # to assign repective column names to the dataframes\n",
    "    transformed_measures_df_less_than_30_percent.columns = measures_df_less_than_30_percent_column_names\n",
    "    transformed_measures_df_less_than_75_percent.columns = measures_df_less_than_75_percent_column_names\n",
    "\n",
    "\n",
    "    '''\n",
    "    to merge the measures data as it has completed its DATA-PREPROCESSING STAGE\n",
    "    '''\n",
    "    final_measures_data = measures_df_less_than_5_percent.merge(transformed_measures_df_less_than_30_percent, right_index=True, left_index=True)\n",
    "    final_measures_data = final_measures_data.merge(transformed_measures_df_less_than_75_percent, right_index=True, left_index=True)\n",
    "\n",
    "\n",
    "    '''\n",
    "    to start PRE-PROCESSING the HISTOGRAM DATA\n",
    "    '''\n",
    "    histogram_data_column_names = histogram_data.columns\n",
    "\n",
    "    # to scale the \"histogram data\" using histogram_data_scaler\n",
    "    final_histogram_data = histogram_data_scaler.transform(histogram_data)\n",
    "\n",
    "    \n",
    "    # to convert the histogram data into a DATAFRAME\n",
    "    final_histogram_data = pd.DataFrame(final_histogram_data, index = measures_df_less_than_5_percent.index)\n",
    "\n",
    "    \n",
    "    # to assign column names to the dataframe\n",
    "    final_histogram_data.columns = histogram_data_column_names\n",
    "\n",
    "    '''\n",
    "    as in the HISTOGRAM DATA, we had only around 1% of the values as NAN values and we removed those, we will now\n",
    "    replace those values with 0 because most of them have a value of 0 and only a few of them have a very large value\n",
    "    '''\n",
    "    final_histogram_data = final_histogram_data.replace(np.NAN, 0)\n",
    "\n",
    "\n",
    "    '''\n",
    "    As we have finished the pre-processing part for histogram data as well, we will now combine both the \"measures\" as\n",
    "    well as \"histogram\" data to create the final input data\n",
    "    '''\n",
    "    final_input_data = final_measures_data.merge(final_histogram_data, left_index=True, right_index=True)\n",
    "\n",
    "    return final_input_data\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "TO DEFINE THE BEHAVIOUR OF THE FLASK API\n",
    "'''\n",
    "@Predictive_equipment_failure_model_productionization.route('/')\n",
    "def hello_world():\n",
    "    return '''Please use the word \"index\" after \"/\"'''\n",
    "\n",
    "\n",
    "@Predictive_equipment_failure_model_productionization.route('/index')\n",
    "def index():\n",
    "    return flask.render_template('index.html')\n",
    "\n",
    "\n",
    "@Predictive_equipment_failure_model_productionization.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    to_predict_list = request.form.to_dict()\n",
    "    \n",
    "    # to collect the input taken from the user (FROM WEB PAGE)\n",
    "    data_points = to_predict_list['review_text']\n",
    "    \n",
    "    \n",
    "    # to separate multiple datapoints given as input by the \":\" symbol\n",
    "    data_points = data_points.split(':')\n",
    "    \n",
    "    \n",
    "    # to separate each datapoint based on \",\" to create a corresponding list\n",
    "    data_points_list = []\n",
    "    \n",
    "    for i in data_points:\n",
    "        values = i.split(',')\n",
    "        data_points_list.append(values)\n",
    "\n",
    "        \n",
    "    # to convert it into DataFrame    \n",
    "    input_df = pd.DataFrame(data_points_list)\n",
    "    input_df.columns = column_names_for_input_data\n",
    "    \n",
    "    \n",
    "    # to replace all the alphabetic inputs by np.NAN\n",
    "    input_df = input_df.replace(r'[^-]\\D', np.NAN, regex=True)\n",
    "    \n",
    "    \n",
    "    # to convert the datapoint into type \"float\"\n",
    "    input_df = input_df.astype('float')\n",
    "    \n",
    "    \n",
    "    # to check for number of negative values in each column\n",
    "    output = input_df.lt(0).sum()\n",
    "    \n",
    "    \n",
    "    # to create a temporary list that store the index value of we have a negative value in the dataframe\n",
    "    temp_list = []\n",
    "    \n",
    "    for i, value in enumerate(output):\n",
    "        if value > 0:\n",
    "            temp_list.append(i)\n",
    "            \n",
    "                    \n",
    "    if len(temp_list) > 0:\n",
    "        return 'Please enter POSITIVE VALUES or NAN VALUES for features {}'.format([i+1 for i in temp_list])\n",
    "    \n",
    "    \n",
    "    # to pass the datapoint into the cleaning data pipeline only if we have the non-negative numbers as inputs\n",
    "    if len(temp_list) == 0:\n",
    "        final_input_data = clean_input_data(input_df, etr_itr_imputer, ridge_regression_imputer, histogram_data_scaler, loaded_base_models_list, meta_classifier, column_names_less_than_5_percent, column_names_for_etr_imputer, column_names_for_ridge_regression_imputer, measures_data_scaler, column_names_per_base_model, top_25_column_names)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Before passing the data-point to the base-estimators, each base-estimator takes 59 columns from the input data\n",
    "        point. These column names for each base estimators are fixed based and so we have stored those column names\n",
    "        and are utilizing those to extract the perfect dataset corresponding to each base estimator\n",
    "        '''\n",
    "        sampled_datasets_list = []\n",
    "\n",
    "        for i in range(500):\n",
    "            sampled_data_points = final_input_data[column_names_per_base_model[i]]\n",
    "            sampled_datasets_list.append(sampled_data_points)\n",
    "\n",
    "        '''\n",
    "        now to pass this data through the base models and extract the outputs of these base models and store them in an\n",
    "        array. This array will then act as the input data for the \"meta_classifier\"\n",
    "        '''\n",
    "        results_from_base_model = []\n",
    "\n",
    "        # to pass this \"final_input_data\" into the base models\n",
    "        for i, base_model in enumerate(loaded_base_models_list):\n",
    "            prediction = base_model.predict(sampled_datasets_list[i])\n",
    "            results_from_base_model.append(prediction)\n",
    "\n",
    "\n",
    "        dataset_for_meta_classifier = np.vstack(results_from_base_model)\n",
    "        dataset_for_meta_classifier = dataset_for_meta_classifier.T\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        now to pass this \"dataset_for_meta_classifier\" as input to the \"META-CLASSIFIER\" and get the FINAL OUTPUT\n",
    "        '''\n",
    "\n",
    "        meta_classifier_output = meta_classifier.predict(dataset_for_meta_classifier)\n",
    "        \n",
    "        \n",
    "        output_list = []\n",
    "        \n",
    "        for i in meta_classifier_output:\n",
    "            if i == 0:\n",
    "                output_list.append('We have SURFACE FAILURE')\n",
    "            if i == 1:\n",
    "                output_list.append('We have DOWNHOLE FAILURE')\n",
    "                \n",
    "        \n",
    "    return \"{}\".format([i for i in output_list])\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Predictive_equipment_failure_model_productionization.run(host='0.0.0.0', port=8080)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
